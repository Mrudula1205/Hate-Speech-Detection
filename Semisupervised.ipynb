{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86657a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1494402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3092f6e95e694455be6b031cd715e59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62385f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'training set.csv', 'test': 'testing set.csv'}\n",
    "df1 = pd.read_csv(\"hf://datasets/thefrankhsu/hate_speech_twitter/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e335656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['categories'] = df1['categories'].fillna(\"Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434ce95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula\\AppData\\Local\\Temp\\ipykernel_23044\\1376013957.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['categories'] = df1['categories'].replace({'Normal':0, 'Race':1, 'Sexual Orientation':2, 'Gender':3, 'Physical Appearance':4, 'Religion':5, 'Disability':6, 'Class':7, 'Ethnicity':8, 'Behavior':9})\n"
     ]
    }
   ],
   "source": [
    "df1['categories'] = df1['categories'].replace({'Normal':0, 'Race':1, 'Sexual Orientation':2, 'Gender':3, 'Physical Appearance':4, 'Religion':5, 'Disability':6, 'Class':7, 'Ethnicity':8, 'Behavior':9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b21541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec49a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "remove_words = list(set(stop_words)-set(['no','never']))\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    clean_text = [word for word in text if not word in remove_words]\n",
    "    return clean_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff90a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mrudula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mrudula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mrudula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mrudula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mrudula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: for additional language support\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if POS is unknown\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Get POS tags for each word\n",
    "    pos_tags = pos_tag(words)\n",
    "    # Lemmatize each word with the correct POS tag\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b96beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "replacement_map = {\n",
    "    r'\\bfuck(ing|in|ry)?\\b': 'fuck',\n",
    "    r'\\bn(igg|igga|iggah|ighas)\\b': 'nigger',\n",
    "    r'\\bfag(got)?\\b': 'faggot'\n",
    "}\n",
    "\n",
    "# Function to apply replacements\n",
    "def replace_words(text):\n",
    "    for pattern, replacement in replacement_map.items():\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77bb5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(text):\n",
    "    '''Function to handle the diacritics in the text'''\n",
    "    import unicodedata\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "df1['Tweet'] = df1['tweet'].astype('str').apply(simplify)\n",
    "df1['Tweet'] = df1['Tweet'].replace(r'@\\w+','',regex=True)\n",
    "df1['Tokenized_tweet'] = df1['Tweet'].apply(tokenizer.tokenize)\n",
    "df1['Clean_tweets'] = df1['Tokenized_tweet'].apply(remove_stopwords)\n",
    "df1['Clean_tweets'] = df1['Clean_tweets'].str.join(' ')\n",
    "df1['Lemmatized_Tweets'] = df1['Clean_tweets'].apply(lemmatize_sentence)\n",
    "df1['Lemmatized_Tweets'] = df1['Lemmatized_Tweets'].apply(replace_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c65bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled = df1['Lemmatized_Tweets']\n",
    "y_labeled = df1['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73258fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('instagram_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f478ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f1e757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    \"fuck\": [\"fuckhes\", \"fu**s\", \"eff\", \"f\", \"fook\", \"falk\", \"phuck\"],\n",
    "    \"fucked\": [\"fcked\"],\n",
    "    \"as fuck\": [\"af\"],\n",
    "    \"shit\": [\"sh$t\", \"sh!t\", \"s**t\"],\n",
    "    \"what the fuck\": [\"wtf\", \"wtfff\", \"dafuq\"],\n",
    "    \"bitch\": [\"b*tch\", \"bich\", \"b*tches\"],\n",
    "    \"get the fuck off\": [\"gtfo\"],\n",
    "    \"fuckers\": [\"f*ckers\"],\n",
    "    \"what the actual fuck\": [\"wtaf\"],\n",
    "    \"fuck this\": [\"fuckthis\"],\n",
    "    \"fuck justin\": [\"fuckjustin\"],\n",
    "    \"fuck him\": [\"fuckhim\"],\n",
    "    \"shame saudi arabia\": [\"shamesaudiarabia\"],\n",
    "    \"the fuck\": [\"tf\"],\n",
    "    \"sucking\": [\"su*king\"],\n",
    "    \"dick\": [\"di*k\"],\n",
    "    \"fucking\": [\"fuckin\", \"effin\", \"effing\", \"fn\"],\n",
    "    \"fuck your\": [\"fuckyour\"],\n",
    "    \"sucks\": [\"s*cks\"],\n",
    "    \"trash\": [\"tra$hh\"],\n",
    "    \"pussy\": [\"pu$$y\"],\n",
    "    \"third\": [\"3rd\"],\n",
    "    \"you have\": [\"you've\"],\n",
    "    \"did not\": [\"didn't\"],\n",
    "    \"do not\": [\"don't\"],\n",
    "    \"were not\": [\"weren't\"],\n",
    "    \"where is\": [\"where's\"],\n",
    "    \"fuck democracy\": [\"fuckdemocracy\"],\n",
    "    \"gaza\": [\"g@.za\"],\n",
    "    \"nigger\": [\"nigga\",\"niggas\"],\n",
    "    \"mother fucker\": [\"mf\"],\n",
    "    \"please\": [\"plz\"],\n",
    "    \"you\": [\"u\"],\n",
    "    \"you are\": [\"you're\"],\n",
    "    \"end israeli apartheid\": [\"endisraeliapartheid\"],\n",
    "    \"palestine\": [\"palest1ne\"],\n",
    "    \"i don't know\": [\"idk\"],\n",
    "    \"to be honest\": [\"tbh\"],\n",
    "    \"your\": [\"ur\"],\n",
    "    \"you have\": [\"you've\"],\n",
    "    \"are not\": [\"aren't\"],\n",
    "    \"nepotism sucks\": [\"nepotismsucks\"],\n",
    "    \"what the hell\": [\"wth\"],\n",
    "    \"be kind\": [\"bekind\"],\n",
    "    \"stop trolling\": [\"stoptrolling\"],\n",
    "    \"i have\": [\"i've\"],\n",
    "    \"was not\": [\"wasn't\"],\n",
    "    \"shut the fuck up\": [\"stfu\"],\n",
    "    \"i know\": [\"ik\"],\n",
    "    \"you all\": [\"y'all\"],\n",
    "    \"ass hole\": [\"a.hole\", \"a-hole\"],\n",
    "    \"rappist\": [\"rappst\"],\n",
    "    \"gay\": [\"g@ys\"],\n",
    "    \"pedophile\": [\"pedo\"],\n",
    "    \"hate\": [\"h8te\"],\n",
    "    \"zionists\": [\"zi0nists\"],\n",
    "    \"holocaust\": [\"hol0caust\"]\n",
    "}\n",
    "def replace_words(text, replacement_map):\n",
    "    # Sort variations by length (longer first) to handle overlapping replacements\n",
    "    sorted_map = {}\n",
    "    for key, variations in replacement_map.items():\n",
    "        for variation in sorted(variations, key=len, reverse=True):\n",
    "            sorted_map[variation] = key\n",
    "\n",
    "    # Create a single regex pattern for all variations\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(var) for var in sorted_map.keys()) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace matches using the sorted map\n",
    "    def replace_match(match):\n",
    "        matched_text = match.group(0).lower()  # Match case-insensitively\n",
    "        return sorted_map.get(matched_text, matched_text)\n",
    "\n",
    "    return pattern.sub(replace_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = list(set(stop_words)-set(['never','no','not']))\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    clean_text = [word for word in text if not word in remove_words]\n",
    "    return clean_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a13531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Text'] = df2['Comment'].apply(lambda x: convert_to_lower(x))\n",
    "df2['Text'] = df2['Text'].apply(lambda x: x.strip().replace('\\n', ' ').replace('\\r', ' '))\n",
    "df2['Text'] = df2['Text'].apply(lambda x: re.sub(r'&#039;', '\\'', x))\n",
    "df2['Text'] = df2['Text'].apply(lambda x: re.sub(r'&[^;]*;', '', x))\n",
    "df2['Text'] = df2['Text'] \\\n",
    "    .apply(lambda x: re.sub(r'[^a-zA-Z\\'\\.\\,]', ' ', x)) \\\n",
    "    .apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x)) \\\n",
    "    .apply(lambda x: re.sub(r'\\.{2,}', '.', x))\n",
    "df2['Text'] = df2['Text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df2['Cleaner_Text'] = df2['Text'].str.lower().apply(tokenizer.tokenize)\n",
    "df2['Clean_tweets'] = df2['Cleaner_Text'].apply(remove_stopwords)\n",
    "df2['Clean_tweets'] = df2['Clean_tweets'].str.join(' ')\n",
    "df2['Lemmatized_Tweets'] = df2['Clean_tweets'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1fdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Categories'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ddc6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unlabeled = df2['Lemmatized_Tweets']\n",
    "y_unlabeled = df2['Categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d850b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aebee403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f97cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier(**sdg_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Unsupervised\n",
    "st_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SelfTrainingClassifier(SGDClassifier(**sdg_params))),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ls_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        # LabelSpreading does not support dense matrices\n",
    "       # (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "        (\"clf\", LabelSpreading(kernel=\"knn\", alpha=0.9)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2bcc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"Number of training samples:\", len(X_train))\n",
    "    print(\"Unlabeled samples in training set:\", sum(1 for x in y_train if x == -1))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\n",
    "        \"Micro-averaged F1 score on test set: %0.3f\"\n",
    "        % f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    )\n",
    "    print(\"-\" * 10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5f632ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([pd.Series(X_labeled), pd.Series(X_unlabeled)], ignore_index=True)\n",
    "y_combined = np.concatenate([y_labeled, y_unlabeled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4d738c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised SGDClassifier on 100% of the data:\n",
      "Number of training samples: 8518\n",
      "Unlabeled samples in training set: 4251\n",
      "Micro-averaged F1 score on test set: 0.883\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.91      0.98      0.94      1428\n",
      "           0       0.90      0.90      0.90      1034\n",
      "           1       0.73      0.59      0.65       147\n",
      "           2       0.96      0.87      0.91       101\n",
      "           3       0.61      0.52      0.56        67\n",
      "           4       0.25      0.08      0.12        13\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.50      0.29      0.36         7\n",
      "           7       1.00      0.27      0.42        15\n",
      "           8       0.00      0.00      0.00         9\n",
      "           9       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.89      2840\n",
      "   macro avg       0.53      0.41      0.44      2840\n",
      "weighted avg       0.88      0.89      0.88      2840\n",
      "\n",
      "Supervised SGDClassifier on 20% of the training data:\n",
      "Number of training samples: 1728\n",
      "Unlabeled samples in training set: 870\n",
      "Micro-averaged F1 score on test set: 0.789\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.90      0.86      1428\n",
      "           0       0.82      0.77      0.79      1034\n",
      "           1       0.59      0.56      0.57       147\n",
      "           2       0.88      0.68      0.77       101\n",
      "           3       0.45      0.28      0.35        67\n",
      "           4       0.33      0.08      0.12        13\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.40      0.29      0.33         7\n",
      "           7       0.50      0.13      0.21        15\n",
      "           8       0.00      0.00      0.00         9\n",
      "           9       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.80      2840\n",
      "   macro avg       0.44      0.34      0.36      2840\n",
      "weighted avg       0.79      0.80      0.79      2840\n",
      "\n",
      "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
      "Number of training samples: 8518\n",
      "Unlabeled samples in training set: 7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 score on test set: 0.274\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00      1428\n",
      "           0       0.39      0.99      0.56      1034\n",
      "           1       0.74      0.46      0.57       147\n",
      "           2       0.96      0.73      0.83       101\n",
      "           3       0.57      0.36      0.44        67\n",
      "           4       0.00      0.00      0.00        13\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.20      0.14      0.17         7\n",
      "           7       0.00      0.00      0.00        15\n",
      "           8       1.00      0.11      0.20         9\n",
      "           9       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.42      2840\n",
      "   macro avg       0.35      0.25      0.25      2840\n",
      "weighted avg       0.23      0.42      0.27      2840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = X_combined, y_combined\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
    "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "    # select a mask of 20% of the train dataset\n",
    "    y_mask = np.random.rand(len(y_train)) < 0.2\n",
    "\n",
    "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
    "    X_20, y_20 = map(list, zip(*((x, y)\n",
    "                     for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
    "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
    "    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "    # set the non-masked subset to be unlabeled\n",
    "    y_train[~y_mask] = -1\n",
    "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
    "          \"is unlabeled):\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, st_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "825a5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdg_params = dict(alpha=1e-5, penalty=\"l2\", loss=\"log_loss\")\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "sdg_params = {\"loss\": \"log_loss\", \"max_iter\": 1000, \"tol\": 1e-3, \"random_state\": 42}\n",
    "rf_params = {\"n_estimators\": 100, \"max_depth\": 20, \"random_state\": 42}\n",
    "svc_params = {\"kernel\": \"linear\", \"probability\": True, \"C\": 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0407141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a553389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SGDClassifier(**sdg_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Unsupervised\n",
    "st_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", SelfTrainingClassifier(RandomForestClassifier(**rf_params))),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ls_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        # LabelSpreading does not support dense matrices\n",
    "       # (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "        (\"clf\", LabelSpreading(kernel=\"knn\", alpha=0.8)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd0d5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised SGDClassifier on 100% of the data:\n",
      "Number of training samples: 8518\n",
      "Unlabeled samples in training set: 4258\n",
      "Micro-averaged F1 score on test set: 0.869\n",
      "----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.98      0.93      1421\n",
      "           0       0.90      0.89      0.89      1033\n",
      "           1       0.74      0.47      0.58       123\n",
      "           2       0.97      0.85      0.90       110\n",
      "           3       0.72      0.54      0.61        80\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.75      0.27      0.40        11\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.88      2840\n",
      "   macro avg       0.45      0.36      0.39      2840\n",
      "weighted avg       0.86      0.88      0.87      2840\n",
      "\n",
      "Supervised SGDClassifier on 20% of the training data:\n",
      "Number of training samples: 1709\n",
      "Unlabeled samples in training set: 865\n",
      "Micro-averaged F1 score on test set: 0.804\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.95      0.88      1421\n",
      "           0       0.83      0.78      0.80      1033\n",
      "           1       0.76      0.45      0.56       123\n",
      "           2       0.94      0.75      0.84       110\n",
      "           3       0.59      0.42      0.49        80\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       1.00      0.27      0.43        11\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.82      2840\n",
      "   macro avg       0.45      0.33      0.36      2840\n",
      "weighted avg       0.80      0.82      0.80      2840\n",
      "\n",
      "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
      "Number of training samples: 8518\n",
      "Unlabeled samples in training set: 7674\n",
      "Micro-averaged F1 score on test set: 0.206\n",
      "----------\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00      1421\n",
      "           0       0.37      1.00      0.54      1033\n",
      "           1       1.00      0.02      0.03       123\n",
      "           2       1.00      0.15      0.25       110\n",
      "           3       0.00      0.00      0.00        80\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.00      0.00      0.00        11\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.37      2840\n",
      "   macro avg       0.22      0.11      0.07      2840\n",
      "weighted avg       0.22      0.37      0.21      2840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mrudula\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = X_combined, y_combined\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
    "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "    # select a mask of 20% of the train dataset\n",
    "    y_mask = np.random.rand(len(y_train)) < 0.2\n",
    "\n",
    "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
    "    X_20, y_20 = map(list, zip(*((x, y)\n",
    "                     for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
    "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
    "    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "    # set the non-masked subset to be unlabeled\n",
    "    y_train[~y_mask] = -1\n",
    "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
    "          \"is unlabeled):\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, st_pipeline.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
